{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw807G1iD7GtnADFEmW5Vv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savina31/darts/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "iybSgqWkTpjq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVD9Wch-YR30",
        "outputId": "34c39bd7-fc1b-41a9-f991-966cfe974d01"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train\n",
        "!mkdir test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZmoUmp8dkLG",
        "outputId": "f6c798cd-e72b-42b9-9f84-7c6282580cf5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘train’: File exists\n",
            "mkdir: cannot create directory ‘test’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvTE8iGNf0Mr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir_latih = '/content/gdrive/MyDrive/ban/train'\n",
        "dir_test = '/content/gdrive/MyDrive/ban/test'\n",
        "dir_latih_target = '/content/train'\n",
        "dir_test_target = '/content/test'"
      ],
      "metadata": {
        "id": "dC60FZZ5dmvA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "from PIL import ImageFilter\n",
        "\n",
        "height, width = 224, 224\n",
        "batch_size=64\n",
        "\n",
        "\n",
        "def generate_data(DIR):\n",
        "    datagen = ImageDataGenerator(rescale=1./255.)\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        DIR,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        class_mode='binary',\n",
        "        target_size=(height, width),\n",
        "        classes={'Normal': 0, 'Retak': 1,}\n",
        "       \n",
        "    )\n",
        "    return generator\n",
        "\n",
        "TRAINING_DIR = '../input/gdrive/MyDrive/ban/train'\n",
        "TESTING_DIR = '../input/gdrive/MyDrive/ban/test'\n",
        "\n",
        "train_generator = generate_data(TRAINING_DIR)\n",
        "test_generator = generate_data(TESTING_DIR)\n",
        "\n",
        "total_image = np.concatenate([train_generator.labels,test_generator.labels])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddcy6cQCd_95",
        "outputId": "29ccb101-db1a-4e41-b192-87617d9f9091"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3GEqVzNMgWR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "R3s0Fz3UdrfM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image dataset\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_data = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Izw2XVzQaswr",
        "outputId": "525876ac-e266-4104-be92-484a2ba2f2c2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-affeddf4a401>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load image dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_data = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_datagen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_plot(generator,image_numbers):\n",
        "    img_feature = generator[0][0][:image_numbers]\n",
        "    img_label = generator[0][1][:image_numbers]\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    for i in range(image_numbers):\n",
        "        ax = plt.subplot(math.ceil(image_numbers/4),4, i + 1)\n",
        "        plt.imshow(img_feature[i])\n",
        "        plt.title(\"Normal\" if img_label[i] == 0 else \"Retak\" )\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "rZk1JTYFVmyr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "input_shape = (height, width, 3)\n",
        "base_model = tf.keras.applications.vgg16.VGG16(\n",
        "    weights='imagenet', \n",
        "    include_top=False,\n",
        "    input_shape=input_shape\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "model_vgg16 = tf.keras.Sequential()\n",
        "model_vgg16.add(base_model)\n",
        "model_vgg16.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "\n",
        "model_vgg16.add(tf.keras.layers.Flatten())\n",
        "model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model_vgg16.add(tf.keras.layers.Dropout(0.5))\n",
        "model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model_vgg16.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model_vgg16.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model_vgg16.compile(loss='BinarylCrossentropy', \n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model_vgg16.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3HZdDoeV5kw",
        "outputId": "9368e23e-2d5e-46bd-de6d-c559c97ebedf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,912,579\n",
            "Trainable params: 197,891\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model/vgg16_best.h5', monitor='acc', verbose=1, mode='max',save_best_only=True)\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",restore_best_weights=True, patience=5)\n",
        "\n",
        "callbacks_list = [checkpoint,early]\n",
        "\n",
        "history = model_vgg16.fit(\n",
        "        train_generator,\n",
        "        validation_data = test_generator,\n",
        "        #steps_per_epoch=10,\n",
        "        epochs=50, \n",
        "        shuffle=False, \n",
        "        verbose=True,\n",
        "        callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "QNNGvs2YWBnh",
        "outputId": "8611ff76-2a62-46fc-e077-688113b896c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9669d2529305>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = model_vgg16.fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"Asked to retrieve element {idx}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"but the Sequence \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot learning curve\n",
        "def plot_learning_curve(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, label='training acc')\n",
        "    plt.plot(epochs, val_acc, label='validation acc')\n",
        "    plt.legend();\n",
        "    plt.figure();\n",
        "\n",
        "    plt.plot(epochs, loss, label='training loss')\n",
        "    plt.plot(epochs, val_loss, label='validation loss')\n",
        "    plt.legend();\n",
        "\n",
        "plot_learning_curve(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "bn8y69-LWInZ",
        "outputId": "662673a2-32f1-4e50-c3e5-2ac86012ea57"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-41d9b757bb4b>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = model_vgg16.evaluate(train_generator)\n",
        "test_result = model_vgg16.evaluate(test_generator)\n",
        "\n",
        "no_augmented_df = pd.DataFrame(zip(train_result,test_result),columns=['Train','Val'],index=['Loss','Acc'])\n",
        "no_augmented_df"
      ],
      "metadata": {
        "id": "0SBX5B9YWMR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest = np.array([])\n",
        "xtest = []\n",
        "\n",
        "for i in range(math.ceil(len(test_generator.classes)/batch_size)):\n",
        "    xtest.append(test_generator[i][0]) \n",
        "    ytest= np.concatenate((ytest,test_generator[i][-1])) \n",
        "    \n",
        "xtest = np.concatenate((xtest),axis=0)\n",
        "\n",
        "ypred_prob =model_vgg16.predict(xtest)\n",
        "ypred = np.argmax(ypred_prob,axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "hm = sns.heatmap(confusion_matrix(ytest,ypred), annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal','Retak'],yticklabels=['Normal','Retak'])  \n",
        "hm.set(xlabel='Predicted_labels')\n",
        "hm.set(ylabel='True_labels')\n",
        "print(classification_report(ytest,ypred))"
      ],
      "metadata": {
        "id": "qXP4MDtYWSBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract wrong classification index\n",
        "wrong_pred = np.where(ypred!=ytest)[0]\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "for i,n in enumerate(wrong_pred):\n",
        "    ax = plt.subplot(math.ceil(len(wrong_pred)/4),4, i + 1)\n",
        "    plt.imshow(xtest[n])\n",
        "    plt.title(\"Normal\" if ypred[n] == 0 else \"Retak\" if ypred[n] ,color='r')\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Y7W9F91uWewj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_augmented(DIR):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255.,\n",
        "        zoom_range=0.1,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip = True\n",
        "    )\n",
        "    generator = datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        batch_size=batch_size,\n",
        "        seed=42,\n",
        "        class_mode='binary',\n",
        "        target_size=(height, width),\n",
        "        classes={'Normal': 0, 'Retak': 1,}\n",
        "    )\n",
        "    return generator\n",
        "\n",
        "aug_train_generator = generate_data_augmented(TRAINING_DIR)\n",
        "\n",
        "image_plot(aug_train_generator,15)"
      ],
      "metadata": {
        "id": "NPc0xZ8DWr1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model/vgg16_best.h5', monitor='acc', verbose=1, mode='max',save_best_only=True)\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",restore_best_weights=True, patience=10)\n",
        "\n",
        "callbacks_list = [checkpoint,early]\n",
        "\n",
        "history = model_vgg16.fit(\n",
        "        aug_train_generator,\n",
        "        validation_data = test_generator,\n",
        "        #steps_per_epoch=10,\n",
        "        epochs=60, \n",
        "        shuffle=False, \n",
        "        verbose=True,\n",
        "        callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "0AIoBpElWx4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = model_vgg16.evaluate(train_generator)\n",
        "test_result = model_vgg16.evaluate(test_generator)\n",
        "\n",
        "augmented_df = pd.DataFrame(zip(train_result,test_result),columns=['Train','Val'],index=['Loss','Acc'])\n",
        "augmented_df"
      ],
      "metadata": {
        "id": "xOOopo4QW2ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = np.array([])\n",
        "xtrain = []\n",
        "\n",
        "for i in range(math.ceil(len(train_generator.classes)/batch_size)):\n",
        "    xtrain.append(train_generator[i][0]) \n",
        "    ytrain= np.concatenate((ytrain,train_generator[i][-1])) \n",
        "    \n",
        "xtrain = np.concatenate((xtrain),axis=0)\n",
        "\n",
        "ypred_prob =model_vgg16.predict(xtrain)\n",
        "ypred = np.argmax(ypred_prob,axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "hm = sns.heatmap(confusion_matrix(ytrain,ypred), annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal','Retak'],yticklabels=['Normal','Retak'])  \n",
        "hm.set(xlabel='Predicted_labels')\n",
        "hm.set(ylabel='True_labels')\n",
        "print(classification_report(ytrain,ypred))"
      ],
      "metadata": {
        "id": "9Xb0j58sW6aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest = np.array([])\n",
        "xtest = []\n",
        "\n",
        "for i in range(math.ceil(len(test_generator.classes)/batch_size)):\n",
        "    xtest.append(test_generator[i][0]) \n",
        "    ytest= np.concatenate((ytest,test_generator[i][-1])) \n",
        "    \n",
        "xtest = np.concatenate((xtest),axis=0)\n",
        "\n",
        "ypred_prob =model_vgg16.predict(xtest)\n",
        "ypred = np.argmax(ypred_prob,axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "hm = sns.heatmap(confusion_matrix(ytest,ypred), annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal','Retak''],yticklabels=['Normal','Retak'])  \n",
        "hm.set(xlabel='Predicted_labels')\n",
        "hm.set(ylabel='True_labels')\n",
        "print(classification_report(ytest,ypred))"
      ],
      "metadata": {
        "id": "4hIVlKeHXI5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}